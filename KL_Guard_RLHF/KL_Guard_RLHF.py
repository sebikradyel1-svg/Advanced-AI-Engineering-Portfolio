"""
RLHF cu PPO - Versiune UniversalÄƒ CompatibilÄƒ
FuncÈ›ioneazÄƒ cu orice versiune de TRL
Include implementare manualÄƒ a divergenÈ›ei KL pentru control complet
"""
from datasets import load_dataset, Dataset
import torch
import torch.nn.functional as F
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer,
    pipeline,
    GenerationConfig
)
from datasets import load_dataset
import numpy as np
from typing import List, Dict, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

# Import condiÈ›ionat pentru TRL
try:
    import trl

    if hasattr(trl, "trainer") and hasattr(trl.trainer, "ppo_trainer"):
        from trl.trainer.ppo_trainer import PPOTrainer
        from trl.trainer.ppo_config import PPOConfig
        from trl.models.modeling_value_head import AutoModelForCausalLMWithValueHead
        print("âœ… Import TRL modern detectat (>=0.10.0)")
    else:
        from trl import PPOTrainer, PPOConfig
        try:
            from trl.models.modeling_value_head import AutoModelForCausalLMWithValueHead
        except ImportError:
            from trl.models import AutoModelForCausalLMWithValueHead
        print("âœ… Import TRL vechi detectat (<=0.8.x)")

    TRL_AVAILABLE = True
    print("âœ… TRL disponibil È™i PPOTrainer importat corect")

except Exception as e:
    print(f"âš ï¸ TRL nu este instalat sau incompatibil ({e}). Folosesc implementare alternativÄƒ.")
    TRL_AVAILABLE = False

# ===============================================================================
# 1. CONFIGURARE UNIVERSALÄ‚
# ===============================================================================

class UniversalConfig:
    """ConfiguraÈ›ie care funcÈ›ioneazÄƒ cu orice setup"""
    MODEL_NAME = "gpt2"
    LEARNING_RATE = 1e-5
    BATCH_SIZE = 2
    MINI_BATCH_SIZE = 1
    
    KL_COEF = 0.2
    KL_TARGET = 0.01
    EPSILON_CLIP = 0.2
    VALUE_CLIP = 0.2
    
    MAX_LENGTH = 80
    TEMPERATURE = 1.0
    
    NUM_STEPS = 50

# ===============================================================================
# 2. IMPLEMENTARE MANUALÄ‚ KL DIVERGENCE
# ===============================================================================

class KLDivergenceController:
    """Controller manual pentru divergenÈ›a KL"""
    
    def __init__(self, initial_coef: float = 0.2, target: float = 0.01):
        self.coef = initial_coef
        self.target = target
        self.history = []
        
    def compute_kl(self, logits: torch.Tensor, ref_logits: torch.Tensor) -> torch.Tensor:
        """CalculeazÄƒ divergenÈ›a KL Ã®ntre douÄƒ distribuÈ›ii"""
        probs = F.softmax(logits, dim=-1)
        ref_probs = F.softmax(ref_logits, dim=-1)
        
        epsilon = 1e-10
        probs = probs + epsilon
        ref_probs = ref_probs + epsilon
        
        kl = torch.sum(probs * (torch.log(probs) - torch.log(ref_probs)), dim=-1)
        return kl
    
    def update_coefficient(self, current_kl: float):
        """AjusteazÄƒ coeficientul KL bazat pe divergenÈ›a curentÄƒ"""
        self.history.append(current_kl)
        
        if current_kl > self.target * 1.5:
            self.coef = min(self.coef * 1.2, 1.0)
            print(f"ðŸ“ˆ KL prea mare ({current_kl:.4f}), cresc coef la {self.coef:.3f}")
        elif current_kl < self.target / 1.5:
            self.coef = max(self.coef * 0.8, 0.01)
            print(f"ðŸ“‰ KL prea mic ({current_kl:.4f}), scad coef la {self.coef:.3f}")
    
    def get_penalty(self, kl_value: torch.Tensor) -> torch.Tensor:
        return self.coef * kl_value

# ===============================================================================
# 3. REWARD MODEL ROBUST
# ===============================================================================

class RobustRewardModel:
    """Model de recompensÄƒ care funcÈ›ioneazÄƒ cu orice setup"""
    
    def __init__(self):
        try:
            self.sentiment_pipe = pipeline(
                "sentiment-analysis",
                model="distilbert-base-uncased-finetuned-sst-2-english",
                device=-1
            )
            self.use_sentiment = True
            print("âœ… Model de sentiment Ã®ncÄƒrcat")
        except Exception as e:
            print(f"âš ï¸ Nu pot Ã®ncÄƒrca model de sentiment: {e}")
            print("ðŸ“Œ Folosesc reward model simplu bazat pe euristici")
            self.use_sentiment = False
    
    def compute_rewards(self, texts: List[str]) -> torch.Tensor:
        rewards = []
        
        for text in texts:
            if self.use_sentiment:
                try:
                    result = self.sentiment_pipe(text[:512])[0]
                    if result['label'] == 'POSITIVE':
                        reward = result['score'] * 2.0
                    else:
                        reward = -result['score'] * 0.5
                except:
                    reward = 0.0
            else:
                reward = self._heuristic_reward(text)
            
            rewards.append(reward)
        
        return torch.tensor(rewards, dtype=torch.float32)
    
    def _heuristic_reward(self, text: str) -> float:
        positive_words = ['good', 'great', 'excellent', 'amazing', 'wonderful', 
                         'fantastic', 'love', 'perfect', 'beautiful', 'best']
        negative_words = ['bad', 'terrible', 'awful', 'horrible', 'worst', 
                         'hate', 'disgusting', 'disappointing']
        
        text_lower = text.lower()
        positive_score = sum(1 for word in positive_words if word in text_lower)
        negative_score = sum(1 for word in negative_words if word in text_lower)
        
        total_words = len(text.split())
        if total_words > 0:
            score = (positive_score - negative_score) / total_words * 10
            return np.clip(score, -2, 2)
        return 0.0

# ===============================================================================
# 4. TRAINER UNIVERSAL ÃŽMBUNÄ‚TÄ‚ÈšIT - SOLUÈšIE COMPLETÄ‚
# ===============================================================================

class UniversalPPOTrainer:
    """
    Trainer PPO care foloseÈ™te doar implementarea manualÄƒ
    EvitÄƒ complet problemele de compatibilitate TRL
    """
    
    def __init__(self):
        self.config = UniversalConfig()
        self.device = torch.device("cpu")
        print(f"ðŸ–¥ï¸ Folosesc device: {self.device}")
        
        # Tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(self.config.MODEL_NAME)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # Folosim doar implementarea manualÄƒ pentru a evita problemele TRL
        self._init_manual()
        
        # Components comune
        self.reward_model = RobustRewardModel()
        self.kl_controller = KLDivergenceController(
            initial_coef=self.config.KL_COEF,
            target=self.config.KL_TARGET
        )
        
        self.stats = {'rewards': [], 'kl_values': []}
        print("âœ… Sistem manual iniÈ›ializat cu succes - fÄƒrÄƒ dependinÈ›e TRL")

    def _init_manual(self):
        """IniÈ›ializare manualÄƒ fÄƒrÄƒ TRL - cea mai stabilÄƒ abordare"""
        print("ðŸ”§ IniÈ›ializare MANUALÄ‚ - maximÄƒ stabilitate")
        
        # Model principal
        self.model = AutoModelForCausalLM.from_pretrained(
            self.config.MODEL_NAME
        ).to(self.device)
        
        # Model de referinÈ›Äƒ (frozen)
        self.ref_model = AutoModelForCausalLM.from_pretrained(
            self.config.MODEL_NAME
        ).to(self.device)
        
        # ÃŽngheÈ›Äƒm modelul de referinÈ›Äƒ
        for param in self.ref_model.parameters():
            param.requires_grad = False
        self.ref_model.eval()
        
        # Optimizer
        self.optimizer = torch.optim.Adam(
            self.model.parameters(), 
            lr=self.config.LEARNING_RATE
        )
        
        self.use_trl = False

    def generate_responses(self, prompts: List[str]) -> Dict:
        """GenereazÄƒ rÄƒspunsuri cu gestionare corectÄƒ a dimensiunilor"""
    # Tokenizare
        inputs = self.tokenizer(
            prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=20,  # ScÄƒzut pentru stabilitate
            return_attention_mask=True
    )
    
        input_ids = inputs["input_ids"].to(self.device)
        attention_mask = inputs["attention_mask"].to(self.device)
    
    # Generare
        with torch.no_grad():
            outputs = self.model.generate(
                input_ids,
                attention_mask=attention_mask,
                max_length=self.config.MAX_LENGTH,
                temperature=self.config.TEMPERATURE,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id
        )
    
    # Decodare
        responses = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)
    
    # CORECÈšIE: ReturneazÄƒ doar input_ids È™i responses, nu output_ids
        return {
            'input_ids': input_ids,
            'responses': responses,
            'attention_mask': attention_mask
    }

    def compute_kl_penalty(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> Tuple[float, torch.Tensor]:
        """CalculeazÄƒ penalizarea KL Ã®ntre model È™i modelul de referinÈ›Äƒ"""
        with torch.no_grad():
            # ObÈ›ine logits de la ambele modele
            model_outputs = self.model(input_ids, attention_mask=attention_mask)
            ref_outputs = self.ref_model(input_ids, attention_mask=attention_mask)
            
            model_logits = model_outputs.logits if hasattr(model_outputs, 'logits') else model_outputs[0]
            ref_logits = ref_outputs.logits if hasattr(ref_outputs, 'logits') else ref_outputs[0]
            
            # CalculeazÄƒ KL
            kl_divergence = self.kl_controller.compute_kl(model_logits, ref_logits)
            kl_mean = kl_divergence.mean().item()
            
            # Penalizare KL
            kl_penalty = self.kl_controller.get_penalty(kl_divergence.mean())
            
        return kl_mean, kl_penalty

    def compute_advantages_and_returns(self, rewards: torch.Tensor, values: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """CalculeazÄƒ avantaje È™i returns pentru PPO manual"""
        # Implementare simplificatÄƒ
        advantages = rewards - values
        returns = rewards
        
        # Normalizare avantaje
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        return advantages, returns

    def ppo_manual_update(self, batch_data: Dict, rewards: torch.Tensor):
        """Implementare manualÄƒ PPO cu corecÈ›ie pentru dimensiuni"""
        self.model.train()
    
        input_ids = batch_data['input_ids']
        attention_mask = batch_data.get('attention_mask', torch.ones_like(input_ids))
    
    # FoloseÈ™te doar input_ids pentru KL
        model_outputs = self.model(input_ids, attention_mask=attention_mask)
        logits = model_outputs.logits if hasattr(model_outputs, 'logits') else model_outputs[0]
    
        with torch.no_grad():
            ref_outputs = self.ref_model(input_ids, attention_mask=attention_mask)
            ref_logits = ref_outputs.logits if hasattr(ref_outputs, 'logits') else ref_outputs[0]
    
    # CalculeazÄƒ KL
        kl_divergence = self.kl_controller.compute_kl(logits, ref_logits)
        kl_mean = kl_divergence.mean().item()
        kl_penalty = self.kl_controller.get_penalty(kl_divergence.mean())
    
    # Loss simplu bazat pe reward
        reward_loss = -rewards.mean()  # MaximizeazÄƒ reward
    
        total_loss = reward_loss + kl_penalty
    
        self.optimizer.zero_grad()
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
        self.optimizer.step()
    
        return {
        'reward_loss': reward_loss.item(),
        'kl_penalty': kl_penalty.item(),
        'kl_mean': kl_mean,
        'total_loss': total_loss.item()
    }

    def train_step(self, prompts: List[str]) -> Dict:
        """Pas de antrenament manual"""
    # GenereazÄƒ rÄƒspunsuri
        gen_output = self.generate_responses(prompts)
    
    # CalculeazÄƒ recompense
        rewards = self.reward_model.compute_rewards(gen_output['responses'])
    
    # Antrenament PPO manual
        training_data = {
            'input_ids': gen_output['input_ids'],
            'attention_mask': gen_output.get('attention_mask', torch.ones_like(gen_output['input_ids']))
    }
    
        stats = self.ppo_manual_update(training_data, rewards)
    
    # ActualizeazÄƒ coeficientul KL
        self.kl_controller.update_coefficient(stats['kl_mean'])
    
    # SalveazÄƒ statistici
        self.stats['rewards'].append(rewards.mean().item())
        self.stats['kl_values'].append(stats['kl_mean'])
    
    # CORECÈšIE: FoloseÈ™te 'reward_loss' Ã®n loc de 'policy_loss'
        return {
        'mean_reward': rewards.mean().item(),
        'kl_divergence': stats['kl_mean'],
        'kl_coef': self.kl_controller.coef,
        'reward_loss': stats['reward_loss'],  # Schimbat din 'policy_loss'
        'total_loss': stats['total_loss'],
        'example': gen_output['responses'][0][:100] if gen_output['responses'] else ""
    }

    def train(self, num_steps: Optional[int] = None):
        """Antrenament principal"""
        num_steps = num_steps or self.config.NUM_STEPS
        
        print("\nðŸŽ“ ÃŽNCEPE ANTRENAMENTUL MANUAL")
        print("="*60)
        print(f"ðŸ“Š ConfiguraÈ›ie:")
        print(f"   â€¢ Model: {self.config.MODEL_NAME}")
        print(f"   â€¢ PaÈ™i: {num_steps}")
        print(f"   â€¢ Batch size: {self.config.BATCH_SIZE}")
        print(f"   â€¢ KL Target: {self.config.KL_TARGET}")
        print(f"   â€¢ Mod: MANUAL (fÄƒrÄƒ TRL)")
        print("="*60)
        
        # Dataset simplu
        prompts_pool = [
            "The movie was",
            "I think this is",
            "This product is",
            "My experience was",
            "The service was",
            "I feel that",
            "This place is",
            "The food was"
        ]
        
        for step in range(num_steps):
            # SelecteazÄƒ batch random
            batch_idx = np.random.choice(
                len(prompts_pool), 
                min(self.config.BATCH_SIZE, len(prompts_pool)), 
                replace=False
            )
            batch_prompts = [prompts_pool[i] for i in batch_idx]
            
            # Pas de antrenament
            step_stats = self.train_step(batch_prompts)
            
            # Logging periodic
            if step % 5 == 0:  # Mai frecvent pentru debugging
                print(f"\nðŸ“ˆ Pas {step}/{num_steps}")
                print(f"   â€¢ Reward: {step_stats['mean_reward']:.3f}")
                print(f"   â€¢ KL Div: {step_stats['kl_divergence']:.4f}")
                print(f"   â€¢ KL Coef: {step_stats['kl_coef']:.3f}")
                print(f"   â€¢ Reward Loss: {step_stats['reward_loss']:.4f}")
                print(f"   â€¢ Total Loss: {step_stats['total_loss']:.4f}")
                if step_stats['example']:
                    print(f"   â€¢ Exemplu: {step_stats['example']}...")
        
        print("\nâœ… Antrenament manual completat!")
        self.print_final_stats()
    
    def print_final_stats(self):
        """AfiÈ™eazÄƒ statistici finale"""
        print("\nðŸ“Š STATISTICI FINALE")
        print("="*60)
        
        if self.stats['rewards']:
            rewards = self.stats['rewards']
            print(f"Reward mediu total: {np.mean(rewards):.3f}")
            print(f"Reward final (ultimele 5): {np.mean(rewards[-5:]):.3f}")
            print(f"Trend reward: {'ðŸ“ˆ' if rewards[-1] > rewards[0] else 'ðŸ“‰'}")
        
        if self.stats['kl_values']:
            kl_values = self.stats['kl_values']
            print(f"KL Divergence medie: {np.mean(kl_values):.4f}")
            print(f"KL Divergence finalÄƒ: {kl_values[-1]:.4f}")
            print(f"KL Coef final: {self.kl_controller.coef:.3f}")
    
    def evaluate(self, test_prompts: List[str]):
        """Evaluare model"""
        print("\nðŸ§ª EVALUARE FINALÄ‚")
        print("="*60)
        
        for i, prompt in enumerate(test_prompts):
            gen_output = self.generate_responses([prompt])
            response = gen_output['responses'][0]
            rewards = self.reward_model.compute_rewards([response])
            
            print(f"\nðŸ“ Prompt {i+1}: {prompt}")
            print(f"   RÄƒspuns: {response}")
            print(f"   Score: {rewards[0]:.3f}")

# ===============================================================================
# 5. EXPLICAÈšIE DETALIATÄ‚
# ===============================================================================

def explain_kl_in_ppo():
    """ExplicaÈ›ie detaliatÄƒ a rolului KL Ã®n PPO"""
    print("\n" + "="*70)
    print("ðŸ“š DIVERGENÈšA KL ÃŽN PPO - EXPLICAÈšIE COMPLETÄ‚")
    print("="*70)
    
    print("""
ðŸŽ¯ PROBLEMA FUNDAMENTALÄ‚:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FÄƒrÄƒ constrÃ¢ngere KL, modelul poate:

1. POLICY COLLAPSE - text repetitiv
2. REWARD HACKING - scurtÄƒturi artificiale  
3. CATASTROPHIC FORGETTING - pierdere capabilitÄƒÈ›i

ðŸ“ SOLUÈšIA: DIVERGENÈšA KL
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
KL(Ï€||Ï€_ref) = ð”¼[log(Ï€(a|s)) - log(Ï€_ref(a|s))]

IMPLEMENTARE ÃŽN PPO:
L = L_policy - Î² * KL(Ï€||Ï€_ref)

âš™ï¸ MECANISM DE CONTROL ADAPTIV:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if KL > target * 1.5: Î² = Î² * 2  # CreÈ™te constrÃ¢ngerea
if KL < target / 1.5: Î² = Î² / 2  # RelaxeazÄƒ constrÃ¢ngerea

ðŸ“Š INTERPRETARE PRACTICÄ‚:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
KL = 0.001  â†’ Prea conservator
KL = 0.01   â†’ Sweet spot optim
KL = 0.1    â†’ Risc instabilitate
KL = 1.0    â†’ Prea agresiv

ðŸ”¬ EXEMPLU CONCRET:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Original:  "The movie was [good|bad|okay|great]"
DupÄƒ RLHF: "The movie was [excellent|amazing|wonderful|fantastic]"
           â† Diversitate menÈ›inutÄƒ! âœ…
""")
    print("="*70)

# ===============================================================================
# 6. MAIN FUNCTION
# ===============================================================================

def main():
    """FuncÈ›ia principalÄƒ"""
    print("ðŸš€ RLHF CU PPO - VERSIUNE UNIVERSALÄ‚ MANUALÄ‚")
    print("="*60)
    print("ðŸ“Œ Folosim implementare MANUALÄ‚ pentru stabilitate maximÄƒ!")
    print("="*60)
    
    # ExplicÄƒ teoria
    explain_kl_in_ppo()
    
    # Antrenament
    print("\nâš™ï¸ IniÈ›ializare sistem manual...")
    trainer = UniversalPPOTrainer()
    
    print("\nðŸŽ“ Start antrenament manual...")
    trainer.train(num_steps=30)  # Redus pentru demo rapidÄƒ
    
    # Evaluare
    test_prompts = [
        "The movie was",
        "This restaurant is", 
        "I really think",
        "My experience was"
    ]
    trainer.evaluate(test_prompts)
    
    print("\nâœ¨ Succes! Sistemul RLHF manual cu control KL a fost demonstrat.")
    print("\nðŸ’¡ Puncte cheie de reÈ›inut:")
    print("â€¢ Implementare manualÄƒ = stabilitate maximÄƒ")
    print("â€¢ KL divergence previne policy collapse")  
    print("â€¢ Coeficientul adaptiv menÈ›ine echilibrul")

if __name__ == "__main__":
    main()